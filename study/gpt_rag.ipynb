{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "# Step 1: Fetch Wikipedia content\n",
    "search_keyword = wikipedia.search(\"MoE\")[0]  # Getting the first search result\n",
    "data = wikipedia.page(search_keyword).content\n",
    "\n",
    "# Step 2: Prepare the DataFrame for embedding\n",
    "embedded_df = pd.DataFrame({\n",
    "    'title': [search_keyword],\n",
    "    'text': [data]\n",
    "})\n",
    "\n",
    "# Step 3: Generate embeddings\n",
    "embedding_func = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "embedded_df['text_embedding'] = embedded_df['text'].apply(lambda x: embedding_func.embed_query(x))\n",
    "\n",
    "# Step 4: Set up Chroma client and a\n",
    "# d data to vector store\n",
    "persist_dir = \"./chroma_db\"\n",
    "chroma_collection_name = \"Testing\"\n",
    "documents = list(embedded_df['title'].values)\n",
    "texts = list(embedded_df['text'].values)\n",
    "\n",
    "chroma_client = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_func,\n",
    "    collection_name=chroma_collection_name,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# Step 5: Set up LangChain retriever and LLM\n",
    "llm = ChatOpenAI()  # OpenAI API key should be set in environment\n",
    "retriever = chroma_client.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Step 6: Create the QA chain\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "# Step 7: Run the query\n",
    "query = \"Explain about python.\"\n",
    "result = qa.run(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Define Chroma database directory and collection name\n",
    "persist_dir = \"./chroma_db\"\n",
    "chroma_collection_name = \"Testing\"\n",
    "\n",
    "# Step 1: Set up Chroma client with persistent storage\n",
    "embedding_func = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "if os.path.exists(persist_dir):\n",
    "    # Load existing Chroma database if it exists\n",
    "    chroma_client = Chroma(\n",
    "        collection_name=chroma_collection_name,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "else:\n",
    "    # If Chroma database doesn't exist, create a new one with Wikipedia content\n",
    "    search_keyword = wikipedia.search(\"MoE\")[0]  # Getting the first search result\n",
    "    data = wikipedia.page(search_keyword).content\n",
    "\n",
    "    # Prepare the DataFrame for embedding\n",
    "    embedded_df = pd.DataFrame({\n",
    "        'title': [search_keyword],\n",
    "        'text': [data]\n",
    "    })\n",
    "    embedded_df['text_embedding'] = embedded_df['text'].apply(lambda x: embedding_func.embed_query(x))\n",
    "    \n",
    "    # Add data to Chroma\n",
    "    documents = list(embedded_df['title'].values)\n",
    "    texts = list(embedded_df['text'].values)\n",
    "    \n",
    "    chroma_client = Chroma.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embedding_func,\n",
    "        collection_name=chroma_collection_name,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "    chroma_client.persist()  # Save data to database\n",
    "\n",
    "# Step 2: Set up LangChain retriever and LLM\n",
    "llm = ChatOpenAI()  # OpenAI API key should be set in environment\n",
    "retriever = chroma_client.as_retriever(search_kwargs={\"k\": 2}, embedding_function=embedding_func)\n",
    "\n",
    "# Step 3: Create the QA chain\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "# Step 4: Run the query\n",
    "query = \"Explain about python.\"\n",
    "result = qa.run(query)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
