{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "import wikipedia\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword(model, user_prompt):\n",
    "    \"\"\"Extract keyword from user prompt using LLM model\"\"\"\n",
    "\n",
    "    keyword_extract_system_prompt = \"\"\"\n",
    "Think and write your step-by-step reasoning before responding.\n",
    "Please write only the fully spelled-out form of the acronym in English that corresponds to the following user's question, without abbreviations or additional text.\n",
    "If you don't know how to respond, just say false.\n",
    "\"\"\"\n",
    "    \n",
    "    template = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{user_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=['system_prompt', 'user_prompt'], template=template)\n",
    "    keyword = model(prompt.format(system_prompt=keyword_extract_system_prompt, user_prompt=user_prompt)).strip()\n",
    "    \n",
    "    return keyword\n",
    "\n",
    "\n",
    "def get_wikipedia_content(keyword):\n",
    "    \"\"\"Fetch content from Wikipedia based on the keyword\"\"\"\n",
    "    try:\n",
    "        search_results = wikipedia.search(keyword)\n",
    "        if not search_results:\n",
    "            return None\n",
    "        page_content = wikipedia.page(search_results[0]).content\n",
    "        return page_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Wikipedia content: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, user_prompt, content=None):\n",
    "    \"\"\"Generate response using GPT model with optional document content\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "Please write all conversations in Korean(í•œêµ­ì–´).\n",
    "Think and write your step-by-step reasoning before responding.\n",
    "Write the article title using ## in Markdown syntax.\n",
    "\"\"\"\n",
    "    \n",
    "    template = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{user_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=['system_prompt', 'user_prompt'], template=template)\n",
    "    if content:\n",
    "        # Split and embed content if provided\n",
    "        doc = Document(page_content=content)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        all_splits = text_splitter.split_documents([doc])\n",
    "        embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)\n",
    "        \n",
    "        # Use RAG for response generation\n",
    "        qachain = RetrievalQA.from_chain_type(model, retriever=vectorstore.as_retriever())\n",
    "        response = qachain(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt))\n",
    "        return response['result']\n",
    "    else:\n",
    "        # Generate response without additional document\n",
    "        response = model(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt)).strip()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar/Projects/MLB_DiscordBot/venv/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/kar/Projects/MLB_DiscordBot/venv/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching Wikipedia content: \"Python\" may refer to: \n",
      "Pythonidae\n",
      "Python (genus)\n",
      "Python (mythology)\n",
      "Python (programming language)\n",
      "CMU Common Lisp\n",
      "PERQ 3\n",
      "Python of Aenus\n",
      "Python (painter)\n",
      "Python of Byzantium\n",
      "Python of Catana\n",
      "Python Anghelo\n",
      "Python (Efteling)\n",
      "Python (Busch Gardens Tampa Bay)\n",
      "Python (Coney Island, Cincinnati, Ohio)\n",
      "Python (automobile maker)\n",
      "Python (Ford prototype)\n",
      "Python (missile)\n",
      "Python (nuclear primary)\n",
      "Colt Python\n",
      "Python (codename)\n",
      "Python (film)\n",
      "Monty Python\n",
      "Python (Monty) Pictures\n",
      "Timon of Phlius\n",
      "Pithon\n",
      "Pyton\n",
      "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì—†ì´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "ì‘ë‹µ: ## Python ì„¤ëª…\n",
      "\n",
      "Pythonì€ **versatile**í•˜ê³  **easy to learn**ì˜ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ğŸ  ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ë¶„ì„í•˜ê¸°, ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œí•˜ê¸°, ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. íŠ¹íˆ Pythonì´ **ë°˜ë³µì ì¸ ì‘ì—…**ì´ë‚˜ **ë°ì´í„° ì²˜ë¦¬**ì— íš¨ê³¼ì ì´ê¸° ë•Œë¬¸ì— ë§ì€ í”„ë¡œê·¸ë˜ë°ìê°€ ì„ íƒí•©ë‹ˆë‹¤. \n",
      "\n",
      "### Pythonì˜ ì¥ì \n",
      "\n",
      "* **í¸ë¦¬í•œ ì„¤ì¹˜ ë° í™œìš©:** Pythonì€ ê°„ë‹¨í•˜ê²Œ ë‹¤ìš´ë¡œë“œí•˜ê³  ì„¤ì¹˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "* **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹° & í’ë¶€í•œ ë„êµ¬:** Pythonì€ ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ì™€ ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ì§€ì›ë§ì„ ì œê³µí•©ë‹ˆë‹¤. \n",
      "\n",
      "**Pythonì˜ í™œìš© ì˜ˆì‹œ:**\n",
      "\n",
      "* **ë°ì´í„° ë¶„ì„:** ë°ì´í„° ì‹œê°í™”, í†µê³„ ë° ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "* **ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ:** Pythonì€ ì›¹ ì„œë²„ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. \n",
      "* **ë¨¸ì‹  ëŸ¬ë‹:**  Pythonì€ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ë° ë§¤ìš° ì í•©í•˜ë©° ë‹¤ì–‘í•œ AI ê¸°ìˆ ë“¤ì„ ì‹¤ì œë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**ê²°ë¡ ì ìœ¼ë¡œ Pythonì€ ë‹¨ìˆœí•˜ê³  ìœ ì—°í•˜ë©´ì„œë„ ê°•ë ¥í•¨ì„ ë™ì‹œì— ì œê³µí•˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.** ğŸš€\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = Ollama(model='gemma2:2b', stop=[\"<|eot_id|>\"])\n",
    "    \n",
    "    user_input = input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "    keyword = extract_keyword(model, user_input)\n",
    "\n",
    "    if keyword == \"false\":\n",
    "        print(\"í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì—†ì´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        response = generate_response(model, user_input)\n",
    "    else:\n",
    "        content = get_wikipedia_content(keyword)\n",
    "        if content:\n",
    "            print(f\"{keyword}ì— ëŒ€í•œ Wikipedia ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            response = generate_response(model, user_input, content=content)\n",
    "        else:\n",
    "            print(\"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì—†ì´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "            response = generate_response(model, user_input)\n",
    "    \n",
    "    print(\"ì‘ë‹µ:\", response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
